{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b4b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mplt\n",
    "#%matplotlib inline  \n",
    "import nltk\n",
    "import numpy as npy\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.probability import FreqDist\n",
    "#conda install -c conda-forge textblob at Anaconda power prompt\n",
    "#python -m textblob.download_corpora. This and the preceding line installs textblob\n",
    "# run at the Anaconda Power prompt \n",
    "from textblob import TextBlob\n",
    "output_file=open(\"amazon_reviews.csv\",'w',encoding='utf-8')\n",
    "input_file=open(\"Reviews.csv\",'r',encoding='utf-8')\n",
    "lines_to_read=10000\n",
    "for i in range(lines_to_read):\n",
    " line=input_file.readline()\n",
    " output_file.write(line) \n",
    "output_file.close() \n",
    "input_file.close() \n",
    "#There are  many columns which are not useful for our sentiment analysis and\n",
    "# hence it’s better to remove the columns which are not required.\n",
    "# We may also create a separate dataframe containing only the requisite columns.\n",
    "review_data=pd.read_csv(\"amazon_reviews.csv\")\n",
    "review_data.shape  #--- This is to know know the dimension of the dataframe\n",
    "review_data.columns #--This is to know the column names of the dataframe\n",
    "review_data['Score'] #---This is to see the review scores for different items\n",
    "review_data['Score'].value_counts().plot(kind='bar') # This is to plot the bars\n",
    "# for different ratings. The bar plot shows the highest rating of 5.\n",
    "# for different ratings. The bar plot shows the highest rating of 5.\n",
    "def pos_tag(text):\n",
    "    try:\n",
    "        return TextBlob(text).tags\n",
    "    except:\n",
    "        return None\n",
    "review_data['pos'] = review_data['Summary'].apply(pos_tag)\n",
    "review_data.to_csv(\"new_file.csv\", index=False)\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [ word for (word,tag) in blob.tags if tag.startswith(\"JJ\")]\n",
    "\n",
    "\n",
    "review_data['adjectives'] = review_data['Summary'].apply(get_adjectives)\n",
    "review_data.columns\n",
    "# Calculating Sentiment Values\n",
    "\n",
    "\n",
    "review_data=review_data.assign(sentiment_value=review_data['HelpfulnessNumerator']/review_data['HelpfulnessDenominator'])\n",
    "\n",
    "\n",
    "review_data['sentiment_value'] = review_data['sentiment_value'].fillna(0)\n",
    "#review_data.replace(to_replace = npy.nan, value ='Negative')-- can also be \n",
    "#used for this purpose\n",
    "\n",
    "review_data['sentiment_value']=review_data.sentiment_value.astype('int64')\n",
    "review_data.columns\n",
    "\n",
    "review_data.dtypes\n",
    "review_data.loc[review_data.sentiment_value< 1, \"sentiment_value\"] = 0\n",
    "\n",
    "review_data['sentiment_value'].replace(\n",
    "    to_replace=[1],\n",
    "   value='Positive',\n",
    "   inplace=True\n",
    ")\n",
    "review_data['sentiment_value'].replace(\n",
    "    to_replace=[0],\n",
    "   value='Negative',\n",
    "   inplace=True\n",
    ")\n",
    "# This is one way of interpretation\n",
    "#for label, row in review_data.iterrows():\n",
    "  # if review_data.loc[label,'sentiment_value'] == 'Positive':\n",
    "   # print(\"Positively Reviewed\")\n",
    " #  else:\n",
    "   # print(\"Negatively Reviewed\")   \n",
    " \n",
    "# This is a better alternative\n",
    "for ind in review_data.index:\n",
    "    if review_data['sentiment_value'][ind]=='Positive':\n",
    "        print(review_data['sentiment_value'][ind])\n",
    "        print(\"The product with id:\",review_data['ProductId'][ind],\"has been  Positively reviewed\")\n",
    "    else:\n",
    "        review_data['sentiment_value'][ind]\n",
    "        print(\"The product with id:\",review_data['ProductId'][ind],\"has been Negatively reviewed\")\n",
    "#review_data.columns\n",
    "review_data_part = review_data.iloc[:6137]\n",
    "review_data_part[\"sentiment_value\"].value_counts()\n",
    "\n",
    "review_df = review_data[['adjectives','sentiment_value']]\n",
    "#review_df\n",
    "#The labels for this dataset are categorical. Machines understand only numeric\n",
    "# data. So, we convert the categorical values to numeric using the factorize()\n",
    "# method. This returns an array of numeric values and an Index of categories.\n",
    "\n",
    "review_text=review_data.loc[:,'adjectives']\n",
    "#review_text.head()\n",
    "# Converting pandas array to numpy array\n",
    "numpy_array = review_text.to_numpy()\n",
    "# The numpy array is now saved in string format\n",
    "npy.savetxt(\"reviews_last.txt\", numpy_array, fmt = \"%s\")\n",
    "# The string data is now read into text.\n",
    "with open(\"reviews_last.txt\",'r') as f:\n",
    "     \n",
    "     text = f.read()\n",
    "f.close()\n",
    "\n",
    " #Passing the string text into word tokenizer\n",
    "# for breaking the sentences\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print(\"The tokenized Text is:\",tokenized_text)\n",
    "for i in tokenized_text:\n",
    "  print (i)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sw= set(stopwords.words('english'))\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)\n",
    "stopwords = [x for x in text1 if x not in sw]\n",
    "print(stopwords)\n",
    "# Word tokenization irrespective of cases.\n",
    "token = word_tokenize(text)\n",
    "print(\"The tokenized Words are:\",token)\n",
    "\n",
    "#From the above output, we can see the text split into tokens. Words, comma, \n",
    "#punctuations are called tokens.\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\") # This is to remove punctuation marks\n",
    "new_words = tokenizer.tokenize(text)\n",
    "print(new_words)\n",
    "review_data_part=review_data_part.assign(final_adjectives=6137)\n",
    "sentiment_label = review_data_part.sentiment_value.factorize()\n",
    "sentiment_label\n",
    "#The 0 here represents positive sentiment and the 1 represents negative\n",
    "# sentiment.\n",
    "#Now, we should transform our text data into something that our machine \n",
    "#learning model understands. Basically, we need to convert the text into an\n",
    "# array of vector embeddings. Word embeddings are a beautiful way of\n",
    "# representing the relationship between the words in the text. To do this,\n",
    "# we first give each of the unique words a unique number and then replace \n",
    "#that word with the number assigned. Now,first, we retrieve all the text data\n",
    "# from the adjective dataset as shown below.\n",
    "#comment = review_df.adjectives.values\n",
    "#print(comment)\n",
    "comments = review_data_part.final_adjectives.values\n",
    "print(comments)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "#tokenizer.fit_on_texts(comments)\n",
    "encoded_docs = tokenizer.texts_to_sequences(comments)\n",
    "padded_sequence = pad_sequences(encoded_docs, maxlen=200)\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_vector_length = 32\n",
    "vocab_size=1000\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "model.add(LSTM(1000, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#Train the sentiment analysis model\n",
    "#Train the sentiment analysis model for 5 epochs on the whole dataset with \n",
    "#a batch size of 32 and a validation split of 20%.\n",
    "# The following command is for training the model and it should be run only once \n",
    "# for a given dataset.for ind in df.index:\n",
    "#The following training has been done. So, you need not run it.     \n",
    "#history = model.fit(padded_sequence,sentiment_label[0],\n",
    "                #  validation_split=0.2, epochs=5, batch_size=32)\n",
    "def predict_sentiment(text):\n",
    "    \n",
    "    \"\"\"tw = tokenizer.texts_to_sequences([text])\n",
    "    tw = pad_sequences(tw,maxlen=200)\n",
    "    prediction = int(model.predict(text).round().item())\"\"\"\n",
    "  \n",
    "    #if text in {'Good','good','great','Great','tasty','fresh','regular','GREAT','Delicious','Best','GOOD','smooth','Awesome','Natural','sensitive','healthy','convenient','wonderful','Excellent','Wonderful','unique','fantastic','favourite','delicious','strong','soft','Organic','Surprising','surprising','cutiest','Adorable','flavourful','fine','beautiful','fair'}:\n",
    "    if sentiment_label[1][0] =='Positive' or text in {'bad','wrong','poor','expensive','gluten','DECEITFUL','Disappointed','artificial','addictive','Addictive','low','little','stale','crap','nasty','insane','hard','slight','outrageous','crazy','fattening','non','small','few'} and text not in {'perfect','able'}: \n",
    "        sentiment='Negative'\n",
    "    else:\n",
    "        sentiment='Positive'\n",
    "    print(\"The expressed Sentiment is: \", sentiment)\n",
    "    \n",
    "\n",
    "\n",
    "for word in comments:\n",
    "   print(\"For the word:\",word) \n",
    "   predict_sentiment(word)\n",
    "   \n",
    "   \n",
    "print(review_data.Score.value_counts())\n",
    "Sentiment_count=review_data.groupby('ProductId').count()\n",
    "print(Sentiment_count)\n",
    "mplt.bar(Sentiment_count.index.values, Sentiment_count['Score'])\n",
    "mplt.xlabel('Review Sentiments')\n",
    "mplt.ylabel('Number of Review')\n",
    "mplt.show()\n",
    "# Generate word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    mplt.figure(figsize=(10, 10))\n",
    "    mplt.imshow(wordcloud) \n",
    "    mplt.axis(\"off\")\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)\n",
    "#Finding frequency distinct in the text\n",
    " \n",
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "\n",
    "fdist = FreqDist(new_words)\n",
    "print(\"The frequency of the tokenized Words:\",fdist)\n",
    "for i in fdist:\n",
    "  print(i)\n",
    "# To find the frequency of top 10 words\n",
    "fdist1= fdist.most_common(10)\n",
    "print(\"The frequency of the top ten words are:\",fdist1)\n",
    "for i in fdist1:\n",
    "  print(i)\n",
    "\n",
    "#Stemming\n",
    " \n",
    "#Stemming usually refers to normalizing words into its base form or root form.\n",
    "\n",
    "# Importing Porterstemmer from nltk library\n",
    "\n",
    "\n",
    "pst = PorterStemmer()\n",
    "\n",
    "# Importing LancasterStemmer from nltk\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "#A list of words to be stemmed\n",
    "word_list = new_words\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,pst.stem(word),lst.stem(word)))\n",
    "\n",
    "#Lancaster is more aggressive than Porter stemmer\n",
    "#Lemmatization\n",
    "#In simpler terms, it is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "#For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to car.\n",
    "\n",
    "#Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP\n",
    "\n",
    "# Importing Lemmatizer library from nltk\n",
    "print(\"The lemmatized  words in the tokenized words are as shown below:\")\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemmatized W\n",
    "                        #Stop Words\n",
    " \n",
    "#“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context. There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n",
    "\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "   print(nltk.pos_tag([token]))\n",
    "\n",
    "#Chunking\n",
    " \n",
    "#Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks.\n",
    "\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = 'NP: {<DT>?<JJ>*<NN>}' \n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)ord\"))\n",
    "print(\"-----------------------------------------\")\n",
    "i=1\n",
    "for word in text:\n",
    "  print(\"{0:20}{1:20}\".format(word,lemmatizer.lemmatize(word)))\n",
    "  i=i+1\n",
    "  if i>10:\n",
    "    break\n",
    "#Stop Words\n",
    " \n",
    "#“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context. There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n",
    "\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "   print(nltk.pos_tag([token]))\n",
    "\n",
    "#Chunking\n",
    " \n",
    "#Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks.\n",
    "\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = 'NP: {<DT>?<JJ>*<NN>}' \n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
